{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3fdb4c",
   "metadata": {},
   "source": [
    "# Property Condition data analysis and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c21c78",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bede1fc",
   "metadata": {},
   "source": [
    "### Looking at the data corresponding to the condition of the property we subset the following columns:\n",
    ">'condition_code', 'grade_code', 'age', 'age_of_renovations', and 'renovated' \n",
    "### We then look at correlations between these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I'll crate some basic variables that will be use through the analysis and modeling\n",
    "df_condition = df_cities[['price','condition_code','grade_code','age','age_of_renovations','renovated']]\n",
    "X_condition = df_condition[['condition_code','grade_code','age','age_of_renovations','renovated']]\n",
    "y_condition = df_condition.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lest look at the feature correlations\n",
    "X_condition.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view this as a heat map to give a better view\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(X_condition.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf5b4a",
   "metadata": {},
   "source": [
    "### We can see that 'age_of_renovations' is closely correlated with 'renovated' and may cause multicolinearity issues. Indeed they are related to the same characteristsic and tell the same tale. We can eliminate one of these from our analysis.\n",
    ">#### If a property is showing a value  > 1 in 'age_of_renovations' then we know it has been renovated which tells us the same as the 'renovated' column... so we will drop 'renovated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_cond = 'price ~ condition_code + grade_code + age + age_of_renovations'\n",
    "\n",
    "condition_model = ols(formula=formula_cond, data=df_condition).fit()\n",
    "condition_model_summ = condition_model.summary()\n",
    "\n",
    "print(condition_model_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf859fe",
   "metadata": {},
   "source": [
    "### Lets take a look at some regressions and see what will give us the strongest model based on the condition variables\n",
    "\n",
    "### *Using all of our features, we get a strong score on both a training data set and also the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_condition, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f4e56",
   "metadata": {},
   "source": [
    "### *We don't need all of these features though so lets iterate through some linear regressions with different feature combinations and see what will give us the simplest, but highest scoring model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'renovated' feature that was giving us multicolinearity issues\n",
    "\n",
    "X_condition_1 = df_condition[['condition_code','grade_code','age','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_1, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b857f69",
   "metadata": {},
   "source": [
    "### *Conditon and Grade tell a similar story about a property. Looking at he data we see Grade is a lot more in depth and provides more bins, so it is potentially more powerful. Lets drop Condition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'condition_code' feature from the model\n",
    "\n",
    "X_condition_2 = df_condition[['grade_code','age','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_2, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593ba1b",
   "metadata": {},
   "source": [
    "### *That barely chaged our model score, and indeed when we look at just Condition it doesn't score well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_3 = df_condition[['condition_code']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_3, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81c014",
   "metadata": {},
   "source": [
    "### *However, looking at just Grade we can see that it scores well and is indeed our strongest feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d41f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_4 = df_condition[['grade_code']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_4, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de977ffb",
   "metadata": {},
   "source": [
    "### *Including the 'age_of_renovations' feature does not add much to the model, so we will drop that as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713aace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_5 = df_condition[['grade_code','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_5, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989e2c",
   "metadata": {},
   "source": [
    "### *Interestingly, Age does not score well by itself. However, when coupled with the Grade feature it gives a large improvement to the overall score.\n",
    "\n",
    "### *This combination of features gives us the highest score, with the least features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bab05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_6 = df_condition[['age']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_6, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_7 = df_condition[['grade_code','age']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_7, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "print(lr.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdf6a5",
   "metadata": {},
   "source": [
    "### Indeed, these are the two columns we will include on the overall \"Linear Regression\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5448a0",
   "metadata": {},
   "source": [
    "## Lets now look at some Polynomial regressions on the \"condition\" data and see if we can improve on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5feae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creat a list of all feature combinations above to iterate through\n",
    "\n",
    "X_conditions = [X_condition, X_condition_1, X_condition_2, X_condition_3, X_condition_4, X_condition_5, X_condition_6, X_condition_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function thet interates through our combinations to determine the the best number of Polynomial Features\n",
    "# It returns an array of train and test scores for each combination\n",
    "\n",
    "def poly_scores_array (X_lst, y):\n",
    "    poly_scores = []\n",
    "    for X in X_lst:\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for i in range(1,7):\n",
    "            poly = PolynomialFeatures(i)\n",
    "            X_poly = pd.DataFrame(poly.fit_transform(X))\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=None, random_state=42)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            train_scores.append(model.score(X_train, y_train))\n",
    "            test_scores.append(model.score(X_test, y_test))\n",
    "            scores = (train_scores, test_scores)\n",
    "            poly_scores.append(scores)\n",
    "    return poly_scores\n",
    "\n",
    "poly_scores_array(X_conditions, y_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f43640",
   "metadata": {},
   "source": [
    "## Just eyeballing the array we conistently get our highest scores up to 4 polynomial features and then our test scores drop significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having determined the best number of polynomial features is 4 for the property condition data\n",
    "# Create a function to iterate through our feature combinations to determine the best model\n",
    "\n",
    "def best_poly_model (X_lst, y):\n",
    "    poly_model_scores = []\n",
    "    for X in X_lst:\n",
    "        poly_2 = PolynomialFeatures(4)\n",
    "        X_poly = pd.DataFrame(poly_2.fit_transform(X))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, y,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42)\n",
    "        lr_poly = LinearRegression()\n",
    "        lr_poly.fit(X_train, y_train)\n",
    "        train_score = lr_poly.score(X_train, y_train)\n",
    "        test_score = lr_poly.score(X_test, y_test)\n",
    "        score = (train_score, test_score)\n",
    "        poly_model_scores.append(score)\n",
    "    return poly_model_scores\n",
    "\n",
    "best_poly_model (X_conditions, y_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be50df0",
   "metadata": {},
   "source": [
    "### The array above corresponds to our feature combinations. In our linear regression analysis we determined that X_condition_7 gave us our best model and we can see above that the same combination gives us excellent results and even scores the best on the test data overall.\n",
    "\n",
    "### Lets see this isolated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_7 = PolynomialFeatures(4)\n",
    "X_poly_7 = pd.DataFrame(poly_7.fit_transform(X_condition_7))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_7, y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42)\n",
    "lr_poly_7 = LinearRegression()\n",
    "lr_poly_7.fit(X_train, y_train)\n",
    "lr_poly_7.score(X_train, y_train)\n",
    "lr_poly_7.score(X_test, y_test)\n",
    "y_hat_train = lr_poly_7.predict(X_train)\n",
    "y_hat_test = lr_poly_7.predict(X_test)\n",
    "\n",
    "print(lr_poly_7.score(X_train, y_train), np.sqrt(mean_squared_error(y_train, y_hat_train)))\n",
    "print(lr_poly_7.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_hat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16c2fb",
   "metadata": {},
   "source": [
    "## **After performing both linear and ploynomial regression analysis on the Property Condition data, we have determined that _Grade (as coded in 'grade_code') and Age are the best variables to use in our model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319ac3e",
   "metadata": {},
   "source": [
    "## Lets take a look at how our model performs on our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8409270",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.linspace(0,2000000)\n",
    "fig,axs = plt.subplots(figsize = (12,8))\n",
    "sns.scatterplot(np.concatenate((y_test,y_train)),np.concatenate((y_hat_test, y_hat_train)), marker = \".\" , s = 8,alpha = 1, label = \"Individual house sell price\")\n",
    "axs.plot(x_line, x_line, color =\"red\", label = \"Ideal prediction line\")\n",
    "axs.set_xlim(0,2000000) ; axs.set_ylim(0,2000000)\n",
    "axs.yaxis.set_major_formatter(currency)\n",
    "axs.xaxis.set_major_formatter(currency)\n",
    "axs.set_aspect(\"equal\")\n",
    "axs.set_title(\"House Price vs house predicted price plot\")\n",
    "axs.set_xlabel(\"House sell price\")\n",
    "axs.set_ylabel(\"House predicted sell price\")\n",
    "axs.legend();\n",
    "plt.savefig(\"./images/property_condition_scatter.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
